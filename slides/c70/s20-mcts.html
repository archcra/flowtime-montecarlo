<h1> <a href="https://stanford.edu/~rezab/classes/cme323/S15/projects/montecarlo_search_tree_report.pdf"> MCTS introduction</a> </h1>


<pre>
  The basic MCTS is conceptually very simple.  A tree is build in an incremental and asymmetric
manner.  Each iteration of MCTS consists of four steps:
•Selection:
   Begin with some root R , a tree policy is used to find the most urgent child of R,
then we successively select child till we reach a leaf L.

•Expansion:
  Unless the node L ends game, create one or more node of L and pick one of them, call it C.

•Simulation:
  play random playouts from C.

•Backpropagation:
  Update the information of the nodes in the path from C to R using the result of the random playouts.
</pre>

<p>
backpropagation 英 [bækprɒpə'ɡeɪʃn]  美 [bækprɒpə'ɡeɪʃn] n.反向传播(B-P)
</p>

<p>
  The
tree policy
is the rule which is used to select a node amongst the children of the root.  The
main difficulty in selecting child nodes is maintaining some balance between the exploitation of
deep  variants  after  moves  with  high  average  win  rate  and  the  exploration  of  moves  with  few
simulations.  The idea is we should maximize the cumulative reward by consistently taking the
optimal action.  The simplest and most widely used policy is called UCT which is introduced
by L. Kocsis and Cs.  Szepesv ́ari.  They recommend to choose in each node of the game tree the
move, for which the expression:

</p>
